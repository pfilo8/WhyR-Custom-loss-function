---
title: Custom loss functions for binary classification problems with highly imbalanced dataset <br> using Extreme Gradient Boosted Trees

author: "Bartosz Kolasa, Patryk Wielopolski"
date: "28 September 2019"
output: 
  ioslides_presentation:
    widescreen: true
    css: milligram.css
bibliography: references.bib
nocite: '@*'
---

## About us {.centered}

![Bartek](images/bartek.jpg){width=40%} ![Patryk](images/patryk.jpg){width=40%}

Data Science Team at DataWalk

## Agenda

1. Motivation and problem statement 
2. Theoretical aspect
3. Experiment
4. Implementation challenges
5. Results and conclusions

# Motivation and problem statement

## Motivation 

* Preserving high precision of model predictions with respect to recall
* Highly imbalanced datasets
    + Fraud detection
    + Object detection

## Problem statement

Status quo:

* Binary classification algorithms often underperforms in predicting positive values on highly imbalanced datasets

Goal:

* Improvement of "postive class friendly" performance measure for highly imbalanced datasets in binary classification problems


# Theoretical aspect

## XGBoost recall

Objective of XGBoost model at step $t$

$$ \sum_{i=1}^{n}[g_if_t(x_i) + \frac{1}{2}h_if_t^2(x_i)] + \Omega(f_t) $$

* $g_i$ - first derivative of loss function
* $h_i$ - second derivative of loss function
* $f_t$ - decision tree at step $t$
* $\Omega$ - regularization term

## Custom Loss Functions {.smaller}

* Cross Entropy
$$ L_{CE}=−ylog(\hat{y})+(1−y)log(1−\hat{y}) $$

* Weighted Cross Entropy
$$ L_{WCE}=−Dylog(\hat{y})+(1−y)log(1−\hat{y}) $$
* Focal Loss
$$ L_{FL} = -y(1-\hat{y})^{\gamma}log(\hat{y}) - (1-y)\hat{y}^{\gamma}log(1-\hat{y}) $$
* Bilinear Loss
$$ L_{CE+B}=(1-\alpha)[−ylog(\hat{y})+(1−y)log(1−\hat{y})] + \alpha[yD + \hat{y} -y\hat{y}(1+D)] $$
* Log Bilinear Loss (after transformations equal to Weigted Cross Entropy)

# Experiment

## Experiment descriptions {.smaller}

Dataset:

* Fraud detection use case 
* Real-world dataset from Insurance Industry 
* 118 unnamed features generated by PCA
* Positive class fraction: 0.7%

Metric:

* F1 Score
* AUCPR

Experiment:

* Best AUCPR for all proposed custom loss functions
* 5 fold stratified Cross Validation
* Hyperparameter tuning using MBO

# Implementation challenges

## Implementation

For implementation we used widely known R packages:

* xgboost
* dplyr
* mlr

&nbsp;

<center>
 ![Logos](images/logos.png){width=70%}
</center>


## Contribution

Our contributions:

* Providing essential derivatives for bilinear loss
* Providing calculations about equality of log bilinear loss and weighted cross entropy
* Implementation of custom loss functions
* Implementation of mlr wrapper for XGBoost with custom loss functions
* Implementation of mlr wrapper for AUCPR measure

# Results and conclusions

## Results

```{r echo = FALSE, results = 'asis'}
library(knitr)

results <- data.frame(
  'Loss function' = c('AUCPR'),
  'Cross Entropy' = c(0.0742625),
  'Focal loss' = c(0.0727012),
  'Weighted CE' = c(0.0724779),
  'Bilinear' = c(0.0641854),
  stringsAsFactors = FALSE
)

kable(results)
```

## Conclusions

* Conclusion 1
* Conclusion 2

## {.flexbox .vcenter}

### Thanks for your attention!

### Questions?

## References {.smaller}

