---
title: "WhyR Presentation"
output: html_notebook
---

Outline of presentation:

* Problem statment
* Custom Loss Functions
* Experiment
* Results

TODO:

* Skrócić wyrażenia z loss functions jeśli się da. Ew. wprowadzić stosunek $a_{01}/a_{10}$ (zmienić też niejasne nazwy)
* Policzyć pochodne i hessiany tych wyrażeń
* Zaimplementować je w R i zobaczyć jakie wyniki wychodzą dla XGBoosta
* Zanonimizować dane rzeczywiste (projekt na gitlabie wiadomy, branch Modeling Iter2, Modeling/31_Modeling_MOD_iter2.ipynb, 3/4 komórka zawiera preprocessing danych oraz ścieżkę do pliku)
* Wymyślić metodologię porównywania wyników (proponuję zapinać się na AUCPR)

Custom Loss Functions
---------------------

\begin{equation} \label{eq:bilinear}
  L_{B}=y^TA\hat{y}
\end{equation}

where y denotes the correct output (y is a probability vector).  Similarly, the log-Bilinearloss is defined as:

\begin{equation} \label{eq:logbilinear}
  L_{LB}=−y^TAlog(1−\hat{y})
\end{equation}
Proposition of new loss function:

\begin{equation} 
  L_{CE} + L_{B}= (1−\alpha)L_{CE}+\alpha yTA\hat{y}(3)
\end{equation}


\begin{equation} 
  L_{CE}+L_{B}= (1−\alpha)L_{CE}−\alpha yTAlog(1−\hat{y})
\end{equation}

where $L_{CE}$ is regular cross entropy function:

\begin{equation} 
  L_{CE}=−ylog(\hat{y})+(1−y)log(1−\hat{y}) 
\end{equation}

#### Binary classification

\begin{equation}  
  L_{L} = ya_{10} + \hat{y}a_{01} - y\hat{y}(a_{10}+a_{01}) 
\end{equation}

and (2) is:

\begin{equation} 
  L_{LB} = -log(\hat{y})*y*a_{10} - log(1-\hat{y})*(1-y)*a_{01}
\end{equation}

where $a_{10}$ and $a_{01}$ comes from penalty matrix:

\begin{equation}
  A  = \begin{bmatrix} 0 & a_{01} \\ a_{10} & 0 \end{bmatrix}
\end{equation}

0 on diagonal comes from fact that is model correctly classify observation we do not want to punish a model.


```{r}
#library(devtools)
#install_github("thomasp85/patchwork")

library(ggplot2)
library(latex2exp)
library(patchwork)

bilinear <- function(y_true, y_pred, A) {
  y_true*A[2, 1] + y_pred*A[1, 2] - y_true*y_pred*(A[1, 2] + A[2, 1])
}

logbilinear <- function(y_true, y_pred, A) {
  -y_true*log(y_pred)*A[2, 1] - (1-y_true)*log(1-y_pred)*A[1, 2]
}

cross_entropy <- function(y_true, y_pred){
  -y_true*log(y_pred) - (1-y_true)*log(1-y_pred)
}

cost_functions_comparison <- function(alpha, A){
  t <- seq(from = 0.0001, to = 0.9999, by = 0.0001)
  
  cross_entropy_0 <- sapply(t, function(t){cross_entropy(0, t)})
  cross_entropy_1 <- sapply(t, function(t){cross_entropy(1, t)})
  cross_entropy_plot <- data.frame(t, cross_entropy_0, cross_entropy_1)
  
  bilinear_0 <- sapply(t, function(t){(1-alpha)*cross_entropy(0, t) + alpha*bilinear(0, t, A)})
  bilinear_1 <- sapply(t, function(t){(1-alpha)*cross_entropy(1, t) + alpha*bilinear(1, t, A)})
  bilinear_plot <- data.frame(t, bilinear_0, bilinear_1)
  
  logbilinear_0 <- sapply(t, function(t){(1-alpha)*cross_entropy(0, t) + alpha*logbilinear(0, t, A)})
  logbilinear_1 <- sapply(t, function(t){(1-alpha)*cross_entropy(1, t) + alpha*logbilinear(1, t, A)})
  logbilinear_plot <- data.frame(t, logbilinear_0, logbilinear_1)
  
  ce_plot <- ggplot(cross_entropy_plot, aes(x = t)) + 
    geom_line(aes(y = cross_entropy_0)) + 
    geom_line(aes(y = cross_entropy_1)) +
    xlab(TeX('\\hat{y}')) + 
    ylab('Value of loss function') +
    ggtitle('Cross entropy loss function')

  bl_plot <- ggplot(bilinear_plot, aes(x = t)) + 
    geom_line(aes(y = bilinear_0)) + 
    geom_line(aes(y = bilinear_1)) +
    xlab(TeX('\\hat{y}')) + 
    ylab('Value of loss function') +
    ggtitle('Bilinear loss function')

  lbl_plot <- ggplot(logbilinear_plot, aes(x = t)) + 
    geom_line(aes(y = logbilinear_0)) + 
    geom_line(aes(y = logbilinear_1)) +
    xlab(TeX('\\hat{y}')) + 
    ylab('Value of loss function') +
    ggtitle('Logbilinear loss function')

  ce_plot + bl_plot + lbl_plot 
}

alpha <- 0.5
CostMatrixSymetric <- matrix(c(0, 1, 1, 0), nrow = 2, ncol = 2)
CostMatrixAsymetric <- matrix(c(0, 1, 3, 0), nrow = 2, ncol = 2)
```

```{r}
cost_functions_comparison(alpha, CostMatrixSymetric)
```

```{r}
cost_functions_comparison(alpha, CostMatrixAsymetric)
```


# Other ideas for loss functions.

2)
$$
−(ylog(\hat{y})+(1−y)log^2(1−\hat{y}))
$$
3)

$$Loss(y,\hat{y})=(1−y)(1)(\hat{y})+(y)(−0.7)(\hat{y})+(0.1)(1−\hat{y})$$

Where $\hat{y}=P(y^=1|X)$

References:

* http://proceedings.mlr.press/v74/resheff17a/resheff17a.pdf
* http://neuralnetworksanddeeplearning.com/chap3.html (Pochodne cross-entropy)
* https://www.di.ens.fr/~fbach/bach06a.pdf
* https://datascience.stackexchange.com/questions/27688/asymmetric-cost-function-for-deep-neural-network-binary-classifier?fbclid=IwAR1Y-HGEhnB5kSGe7VkogzRV-FBih30ESNeR0XfNSfUMIXpNfTMVp7QOx1k
